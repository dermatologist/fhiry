{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udd25 fhiry \u2014 FHIR to Pandas DataFrame for Data Analytics, AI, and ML","text":"<p>FHIRy is a Python package that simplifies health data analytics and machine learning by converting FHIR bundles or NDJSON files from bulk data export into pandas DataFrames. These DataFrames can be used directly with ML libraries such as TensorFlow and PyTorch. FHIRy also supports FHIR server search and FHIR tables on BigQuery.</p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Flatten FHIR Bundles/NDJSON to DataFrames for analytics and ML</li> <li>Import from FHIR Server via FHIR Search API</li> <li>Query FHIR Data on Google BigQuery</li> <li>LLM-based Natural Language Queries (see examples/llm_example.py)</li> <li>Flexible Filtering and Column Selection</li> </ul>"},{"location":"#quick-start","title":"\ud83d\udd27 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Stable release:</p> <pre><code>pip install fhiry\n</code></pre> <p>Latest development version:</p> <pre><code>pip install git+https://github.com/dermatologist/fhiry.git\n</code></pre> <p>LLM support:</p> <pre><code>pip install fhiry[llm]\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#1-import-fhir-bundles-json-from-folder","title":"1. Import FHIR Bundles (JSON) from Folder","text":"<pre><code>import fhiry.parallel as fp\ndf = fp.process('/path/to/fhir/resources')\nprint(df.info())\n</code></pre> <p>Example dataset: Synthea Notebook: <code>notebooks/synthea.ipynb</code></p>"},{"location":"#2-import-ndjson-from-folder","title":"2. Import NDJSON from Folder","text":"<pre><code>import fhiry.parallel as fp\ndf = fp.ndjson('/path/to/fhir/ndjson/files')\nprint(df.info())\n</code></pre> <p>Example dataset: SMART Bulk Data Server Notebook: <code>notebooks/ndjson.ipynb</code></p>"},{"location":"#3-import-fhir-search-results","title":"3. Import FHIR Search Results","text":"<p>Fetch resources from a FHIR server using the FHIR Search API:</p> <pre><code>from fhiry.fhirsearch import Fhirsearch\n\nfs = Fhirsearch(fhir_base_url=\"http://fhir-server:8080/fhir\")\nparams = {\"code\": \"http://snomed.info/sct|39065001\"}\ndf = fs.search(resource_type=\"Condition\", search_parameters=params)\nprint(df.info())\n</code></pre> <p>See <code>fhir-search.md</code> for details.</p>"},{"location":"#4-import-from-google-bigquery-fhir-dataset","title":"4. Import from Google BigQuery FHIR Dataset","text":"<pre><code>from fhiry.bqsearch import BQsearch\nbqs = BQsearch()\ndf = bqs.search(\"SELECT * FROM `bigquery-public-data.fhir_synthea.patient` LIMIT 20\")\n</code></pre>"},{"location":"#5-llm-based-natural-language-queries","title":"\ud83d\ude80 5. LLM-based Natural Language Queries","text":"<p>FHIRy supports natural language queries over FHIR bundles/NDJSON using llama-index:</p> <pre><code>pip install fhiry[llm]\n</code></pre> <p>See usage: <code>examples/llm_example.py</code></p>"},{"location":"#6-convert-fhir-bundlesresources-to-text-for-llms","title":"\ud83d\ude80 6. Convert FHIR Bundles/Resources to Text for LLMs","text":"<p>Convert a FHIR Bundle or resource to a textual representation for LLMs:</p> <pre><code>from fhiry import FlattenFhir\nimport json\n\nbundle = json.load(open('bundle.json'))\nflatten_fhir = FlattenFhir(bundle)\nprint(flatten_fhir.flattened)\n</code></pre>"},{"location":"#filters-and-column-selection","title":"Filters and Column Selection","text":"<p>You can pass a config JSON to any constructor to remove or rename columns:</p> <pre><code>df = fp.process('/path/to/fhir/resources', config_json='{ \"REMOVE\": [\"resource.text.div\"], \"RENAME\": { \"resource.id\": \"id\" } }')\nfs = Fhirsearch(fhir_base_url=\"http://fhir-server:8080/fhir\", config_json='{ \"REMOVE\": [\"resource.text.div\"], \"RENAME\": { \"resource.id\": \"id\" } }')\nbqs = BQsearch('{ \"REMOVE\": [\"resource.text.div\"], \"RENAME\": { \"resource.id\": \"id\" } }')\n</code></pre> <p>See <code>df.columns</code> for available columns. Example columns:</p> <pre><code>patientId\nfullUrl\nresource.resourceType\nresource.id\nresource.name\nresource.telecom\nresource.gender\n...\n</code></pre>"},{"location":"#command-line-interface-cli","title":"Command Line Interface (CLI)","text":"<p>See CLI examples:</p> <pre><code>fhiry --help\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation: https://dermatologist.github.io/fhiry/</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! See CONTRIBUTING.md.</p>"},{"location":"#give-us-a-star","title":"Give Us a Star \u2b50\ufe0f","text":"<p>If you find this project useful, please give us a star to help others discover it.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Bell Eapen </li> <li>Markus Mandalka</li> <li>PRs welcome! See CONTRIBUTING.md</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Optimize pandas operations to eliminate O\\(n\u00b2\\) concatenation and vectorize patient ID extraction #228 (Copilot)</li> <li>build\\(deps\\): bump astral-sh/setup-uv from 6 to 7 #227 (dependabot[bot])</li> <li>[Automated] Dependencies upgrade #226 (github-actions[bot])</li> <li>feat: add GitHub Dependents Info workflow and update README with depe\u2026 #221 (dermatologist)</li> <li>build\\(deps\\): bump actions/checkout from 4 to 5 #220 (dependabot[bot])</li> <li>[Automated] Dependencies upgrade #219 (github-actions[bot])</li> <li>fix: update DataFrame column handling to use comma-separated values f\u2026 #218 (dermatologist)</li> <li>Feature/uv 2 #217 (dermatologist)</li> <li>fix: enhance subject reference handling in check_subject_reference me\u2026 #216 (dermatologist)</li> <li>build\\(deps\\): bump actions/checkout from 4 to 5 #215 (dependabot[bot])</li> <li>ci: update pytest workflow schedule and add test for CLI #214 (dermatologist)</li> <li>Feature/command line 1 #213 (dermatologist)</li> <li>fix: update LLM example and dependencies for GoogleGenAI integration #211 (dermatologist)</li> <li>Feature/uv 1 #210 (dermatologist)</li> <li>Feature/uv 1 #209 (dermatologist)</li> <li>build\\(deps\\): bump jinja2 from 3.1.4 to 3.1.6 #203 (dependabot[bot])</li> <li>style: clean up code formatting and improve consistency in string quotes #199 (dermatologist)</li> <li>test: update patient birth date and adjust age calculation in tests #198 (dermatologist)</li> <li>Feature/update deps 1 #194 (dermatologist)</li> <li>build\\(deps\\): bump actions/setup-python from 5.1.1 to 5.3.0 #192 (dependabot[bot])</li> <li>build\\(deps\\): bump actions/setup-python from 5.1.0 to 5.1.1 #187 (dependabot[bot])</li> <li>Bump jinja2 from 3.1.3 to 3.1.4 #180 (dependabot[bot])</li> <li>Bump actions/setup-python from 5.0.0 to 5.1.0 #174 (dependabot[bot])</li> <li>Bump jinja2 from 3.0.1 to 3.1.3 #167 (dependabot[bot])</li> <li>Bump actions/setup-python from 4.1.0 to 5.0.0 #166 (dependabot[bot])</li> <li>Feature/update deps 1 #164 (dermatologist)</li> <li>Feature/update python 1 #162 (dermatologist)</li> <li>fix #135 #160 (dermatologist)</li> <li>Bump pyarrow from 13.0.0 to 14.0.1 #159 (dependabot[bot])</li> <li>Bump actions/checkout from 3 to 4 #150 (dependabot[bot])</li> <li>Update deps #145 (dermatologist)</li> <li>Bump wheel from 0.37.1 to 0.41.0 #142 (dependabot[bot])</li> <li>add utf-8 encoding #134 (fhirfly)</li> </ul>"},{"location":"changelog/#v511-2025-08-16","title":"v5.1.1 (2025-08-16)","text":"<p>Full Changelog</p>"},{"location":"changelog/#v510-2025-08-15","title":"v5.1.0 (2025-08-15)","text":"<p>Full Changelog</p>"},{"location":"changelog/#500-2025-06-15","title":"5.0.0 (2025-06-15)","text":"<p>Full Changelog</p>"},{"location":"changelog/#421-2025-05-04","title":"4.2.1 (2025-05-04)","text":"<p>Full Changelog</p>"},{"location":"changelog/#420-2025-05-04","title":"4.2.0 (2025-05-04)","text":"<p>Full Changelog</p>"},{"location":"changelog/#413-2025-05-01","title":"4.1.3 (2025-05-01)","text":"<p>Full Changelog</p>"},{"location":"changelog/#412-2025-05-01","title":"4.1.2 (2025-05-01)","text":"<p>Full Changelog</p>"},{"location":"changelog/#411-2025-05-01","title":"4.1.1 (2025-05-01)","text":"<p>Full Changelog</p>"},{"location":"changelog/#410-2025-05-01","title":"4.1.0 (2025-05-01)","text":"<p>Full Changelog</p>"},{"location":"changelog/#400-2024-07-03","title":"4.0.0 (2024-07-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#322-2023-12-03","title":"3.2.2 (2023-12-03)","text":"<p>Full Changelog</p>"},{"location":"changelog/#321-2023-11-27","title":"3.2.1 (2023-11-27)","text":"<p>Full Changelog</p>"},{"location":"changelog/#320-2023-11-20","title":"3.2.0 (2023-11-20)","text":"<p>Full Changelog</p>"},{"location":"changelog/#310-2023-11-14","title":"3.1.0 (2023-11-14)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Flattening FHIR resources / bundle for LLMs #144</li> </ul> <p>Closed issues:</p> <ul> <li>Performance warning: DataFrame is highly fragmented #135</li> <li>'charmap' codec can't decode byte 0x81 in position 1603 #133</li> </ul>"},{"location":"changelog/#300-2023-03-09","title":"3.0.0 (2023-03-09)","text":"<p>Full Changelog</p>"},{"location":"changelog/#210-2023-02-06","title":"2.1.0 (2023-02-06)","text":"<p>Full Changelog</p>"},{"location":"changelog/#200-2022-03-17","title":"2.0.0 (2022-03-17)","text":"<p>Full Changelog</p>"},{"location":"changelog/#100-2021-09-17","title":"1.0.0 (2021-09-17)","text":"<p>Full Changelog</p>"},{"location":"changelog/#050-2020-12-26","title":"0.5.0 (2020-12-26)","text":"<p>Full Changelog</p>"},{"location":"changelog/#030-2020-12-21","title":"0.3.0 (2020-12-21)","text":"<p>Full Changelog</p>"},{"location":"changelog/#021-2020-12-15","title":"0.2.1 (2020-12-15)","text":"<p>Full Changelog</p>"},{"location":"changelog/#020-2020-12-15","title":"0.2.0 (2020-12-15)","text":"<p>Full Changelog</p> <p>* This Changelog was automatically generated by github_changelog_generator</p>"},{"location":"contributing/","title":"Contributing to <code>fhiry</code>","text":""},{"location":"contributing/#please-note","title":"Please note:","text":"<ul> <li>(Optional) We adopt Git Flow. Most feature branches are pushed to the repository and deleted when merged to develop branch.</li> <li>(Important): Submit pull requests to the develop branch or feature/ branches</li> <li>Use GitHub Issues for feature requests and bug reports. Include as much information as possible while reporting bugs.</li> </ul>"},{"location":"contributing/#contributing-step-by-step","title":"Contributing (Step-by-step)","text":"<ol> <li> <p>Fork the repo and clone it to your local computer, and set up the upstream remote:</p> <pre><code>git clone https://github.com/dermatologist/fhiry.git\ncd fhiry\ngit remote add upstream https://github.com/dermatologist/fhiry.git\n</code></pre> </li> <li> <p>Checkout out a new local branch based on your master and update it to the latest (BRANCH-123 is the branch name, You can name it whatever you want. Try to give it a meaningful name. If you are fixing an issue, please include the issue #).</p> <pre><code>git checkout -b BRANCH-123 develop\ngit clean -df\ngit pull --rebase upstream develop\n</code></pre> </li> </ol> <p>Please keep your code clean. If you find another bug, you want to fix while being in a new branch, please fix it in a separated branch instead.</p> <ol> <li> <p>Push the branch to your fork. Treat it as a backup.</p> <pre><code>git push origin BRANCH-123\n</code></pre> </li> <li> <p>Code</p> </li> <li> <p>Adhere to common conventions you see in the existing code.</p> </li> <li> <p>Include tests as much as possible, and ensure they pass.</p> </li> <li> <p>Commit to your branch</p> <pre><code> git commit -m \"BRANCH-123: Put change summary here (can be a ticket title)\"\n</code></pre> </li> </ol> <p>NEVER leave the commit message blank! Provide a detailed, clear, and complete description of your commit!</p> <ol> <li> <p>Update your branch to the latest code.</p> <pre><code>git pull --rebase upstream develop\n</code></pre> </li> <li> <p>Important If you have made many commits, please squash them into atomic units of work. (Most Git GUIs such as sourcetree and smartgit offer a squash option)</p> <pre><code>git checkout develop\ngit pull --rebase upstream develop\ngit merge --squash BRANCH-123\ngit commit -m \"fix: 123\"\n</code></pre> </li> </ol> <p>Push changes to your fork:</p> <pre><code>    git push\n</code></pre> <ol> <li>Issue a Pull Request</li> </ol> <p>In order to make a pull request:   * Click \"Pull Request\".   * Choose the develop branch   * Click 'Create pull request'   * Fill in some details about your potential patch including a meaningful title.   * Click \"Create pull request\".</p> <p>Thanks for that -- we'll get to your pull request ASAP. We love pull requests!</p>"},{"location":"contributing/#feedback","title":"Feedback","text":"<p>If you need to contact me, see my contact details on my profile page.</p>"},{"location":"github-dependents-info/","title":"Dependents stats for dermatologist/fhiry","text":"Repository Stars theislab / ehrapy 303 dermatologist / pyomop 48 paulhager / MIMIC-Clinical-Decision-Making-Framework 43 paulhager / MIMIC-Clinical-Decision-Making-Dataset 37 paulhager / MIMIC-Clinical-Decision-Making-Analysis 20 alphavector / all 5 Lyn4ever29 / pipy_server 3 CHARM-BDF / CHARMTwinsights 2 mederrata / pyomop 0 ArsMedicaTech / MIMIC-Clinical-Decision-Making-Framework 0 Techolution / ehrapy_sample_repo 0 rchemist0123 / raremade_delivery_processing 0 dermatologist / dhti-elixir-base 0 <p>Generated using github-dependents-info, by Nicolas Vuillamy</p>"},{"location":"modules/","title":"Modules","text":"<p>Copyright (c) 2020 Bell Eapen</p> <p>This software is released under the MIT License. https://opensource.org/licenses/MIT</p> <p>Copyright (c) 2020 Bell Eapen</p> <p>This software is released under the MIT License. https://opensource.org/licenses/MIT</p> <p>Copyright (c) 2023 Bell Eapen</p> <p>This software is released under the MIT License. https://opensource.org/licenses/MIT</p> <p>Copyright (c) 2024 Bell Eapen</p> <p>This software is released under the MIT License. https://opensource.org/licenses/MIT</p> <p>Copyright (c) 2020 Bell Eapen</p> <p>This software is released under the MIT License. https://opensource.org/licenses/MIT</p> <p>Copyright (c) 2025 Bell Eapen</p> <p>This software is released under the MIT License. https://opensource.org/licenses/MIT</p>"},{"location":"modules/#fhiry.Fhiry","title":"<code>Fhiry</code>","text":"<p>               Bases: <code>BaseFhiry</code></p> <p>Read and process FHIR Bundles (.json) from file or folder.</p> <p>Parameters:</p> Name Type Description Default <code>config_json</code> <p>Optional JSON string or file path with column transforms.</p> <code>None</code> Source code in <code>src/fhiry/fhiry.py</code> <pre><code>class Fhiry(BaseFhiry):\n    \"\"\"Read and process FHIR Bundles (.json) from file or folder.\n\n    Args:\n        config_json: Optional JSON string or file path with column transforms.\n    \"\"\"\n\n    def __init__(self, config_json=None):\n        self._filename = \"\"\n        self._folder = \"\"\n        super().__init__(config_json=config_json)\n\n    @property\n    def df(self):\n        \"\"\"pd.DataFrame | None: The current working dataframe, if any.\"\"\"\n        return self._df\n\n    @property\n    def filename(self):\n        \"\"\"str: The path to the currently selected input file, if any.\"\"\"\n        return self._filename\n\n    @property\n    def folder(self):\n        \"\"\"str: The path to the input folder containing Bundle JSON files.\"\"\"\n        return self._folder\n\n    @property\n    def delete_col_raw_coding(self):\n        \"\"\"bool: Whether to drop raw coding/display columns after extraction.\"\"\"\n        return self._delete_col_raw_coding\n\n    @filename.setter\n    def filename(self, filename):\n        \"\"\"Set the input file and load it into a dataframe.\n\n        Args:\n            filename (str): Path to a FHIR Bundle JSON file.\n        \"\"\"\n        self._filename = filename\n        self._df = self.read_bundle_from_file(filename)\n\n    @folder.setter\n    def folder(self, folder):\n        \"\"\"Set the input folder for processing Bundle JSON files.\n\n        Args:\n            folder (str): Path to a directory containing JSON files.\n        \"\"\"\n        self._folder = folder\n\n    @delete_col_raw_coding.setter\n    def delete_col_raw_coding(self, delete_col_raw_coding):\n        \"\"\"Set whether to drop raw coding/display columns after extraction.\"\"\"\n        self._delete_col_raw_coding = delete_col_raw_coding\n\n    def read_bundle_from_file(self, filename):\n        \"\"\"Load a FHIR Bundle JSON file and normalize its entries.\n\n        Args:\n            filename (str): Path to a FHIR Bundle JSON file.\n\n        Returns:\n            pd.DataFrame: Dataframe of the Bundle entries.\n        \"\"\"\n        with open(filename, encoding=\"utf8\", mode=\"r\") as f:\n            json_in = f.read()\n            json_in = json.loads(json_in)\n            return pd.json_normalize(json_in[\"entry\"])\n\n    def process_source(self):\n        \"\"\"Process either the selected file or the entire folder.\n\n        Only columns common across resources will be mapped.\n        \"\"\"\n        if self._folder:\n            # Collect all dataframes first, then concat once for better performance\n            dataframes = []\n            for file in tqdm(os.listdir(self._folder)):\n                if file.endswith(\".json\"):\n                    self._df = self.read_bundle_from_file(\n                        os.path.join(self._folder, file)\n                    )\n                    self.process_df()\n                    if not self._df.empty:\n                        dataframes.append(self._df)\n\n            # Single concat operation with ignore_index for better performance\n            if dataframes:\n                self._df = pd.concat(dataframes, ignore_index=True)\n            else:\n                self._df = pd.DataFrame()\n        elif self._filename:\n            self._df = self.read_bundle_from_file(self._filename)\n        super().process_df()\n\n    def process_file(self, filename):\n        \"\"\"Process a single Bundle JSON file and return its dataframe.\"\"\"\n        self._df = self.read_bundle_from_file(filename)\n        self.process_df()\n        return self._df\n\n    def process_bundle_dict(self, bundle_dict):\n        \"\"\"Process a FHIR Bundle dictionary and return its dataframe.\"\"\"\n        self._df = self.read_bundle_from_bundle_dict(bundle_dict)\n        self.process_df()\n        return self._df\n</code></pre>"},{"location":"modules/#fhiry.Fhiry.delete_col_raw_coding","title":"<code>delete_col_raw_coding</code>  <code>property</code> <code>writable</code>","text":"<p>bool: Whether to drop raw coding/display columns after extraction.</p>"},{"location":"modules/#fhiry.Fhiry.df","title":"<code>df</code>  <code>property</code>","text":"<p>pd.DataFrame | None: The current working dataframe, if any.</p>"},{"location":"modules/#fhiry.Fhiry.filename","title":"<code>filename</code>  <code>property</code> <code>writable</code>","text":"<p>str: The path to the currently selected input file, if any.</p>"},{"location":"modules/#fhiry.Fhiry.folder","title":"<code>folder</code>  <code>property</code> <code>writable</code>","text":"<p>str: The path to the input folder containing Bundle JSON files.</p>"},{"location":"modules/#fhiry.Fhiry.process_bundle_dict","title":"<code>process_bundle_dict(bundle_dict)</code>","text":"<p>Process a FHIR Bundle dictionary and return its dataframe.</p> Source code in <code>src/fhiry/fhiry.py</code> <pre><code>def process_bundle_dict(self, bundle_dict):\n    \"\"\"Process a FHIR Bundle dictionary and return its dataframe.\"\"\"\n    self._df = self.read_bundle_from_bundle_dict(bundle_dict)\n    self.process_df()\n    return self._df\n</code></pre>"},{"location":"modules/#fhiry.Fhiry.process_file","title":"<code>process_file(filename)</code>","text":"<p>Process a single Bundle JSON file and return its dataframe.</p> Source code in <code>src/fhiry/fhiry.py</code> <pre><code>def process_file(self, filename):\n    \"\"\"Process a single Bundle JSON file and return its dataframe.\"\"\"\n    self._df = self.read_bundle_from_file(filename)\n    self.process_df()\n    return self._df\n</code></pre>"},{"location":"modules/#fhiry.Fhiry.process_source","title":"<code>process_source()</code>","text":"<p>Process either the selected file or the entire folder.</p> <p>Only columns common across resources will be mapped.</p> Source code in <code>src/fhiry/fhiry.py</code> <pre><code>def process_source(self):\n    \"\"\"Process either the selected file or the entire folder.\n\n    Only columns common across resources will be mapped.\n    \"\"\"\n    if self._folder:\n        # Collect all dataframes first, then concat once for better performance\n        dataframes = []\n        for file in tqdm(os.listdir(self._folder)):\n            if file.endswith(\".json\"):\n                self._df = self.read_bundle_from_file(\n                    os.path.join(self._folder, file)\n                )\n                self.process_df()\n                if not self._df.empty:\n                    dataframes.append(self._df)\n\n        # Single concat operation with ignore_index for better performance\n        if dataframes:\n            self._df = pd.concat(dataframes, ignore_index=True)\n        else:\n            self._df = pd.DataFrame()\n    elif self._filename:\n        self._df = self.read_bundle_from_file(self._filename)\n    super().process_df()\n</code></pre>"},{"location":"modules/#fhiry.Fhiry.read_bundle_from_file","title":"<code>read_bundle_from_file(filename)</code>","text":"<p>Load a FHIR Bundle JSON file and normalize its entries.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to a FHIR Bundle JSON file.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: Dataframe of the Bundle entries.</p> Source code in <code>src/fhiry/fhiry.py</code> <pre><code>def read_bundle_from_file(self, filename):\n    \"\"\"Load a FHIR Bundle JSON file and normalize its entries.\n\n    Args:\n        filename (str): Path to a FHIR Bundle JSON file.\n\n    Returns:\n        pd.DataFrame: Dataframe of the Bundle entries.\n    \"\"\"\n    with open(filename, encoding=\"utf8\", mode=\"r\") as f:\n        json_in = f.read()\n        json_in = json.loads(json_in)\n        return pd.json_normalize(json_in[\"entry\"])\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry","title":"<code>BaseFhiry</code>","text":"<p>               Bases: <code>object</code></p> <p>Base class providing common dataframe processing utilities for FHIR.</p> <p>This class encapsulates common logic for transforming FHIR bundle data into a pandas DataFrame, including column cleanup, code extraction, and patient ID derivation.</p> <p>Parameters:</p> Name Type Description Default <code>config_json</code> <p>Either a JSON string or a path to a JSON file specifying transformations with keys: - \"REMOVE\": list[str] of column prefixes to remove - \"RENAME\": dict[str, str] mapping old-&gt;new column names If None, a sensible default is used.</p> <code>None</code> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>class BaseFhiry(object):\n    \"\"\"Base class providing common dataframe processing utilities for FHIR.\n\n    This class encapsulates common logic for transforming FHIR bundle data into\n    a pandas DataFrame, including column cleanup, code extraction, and patient\n    ID derivation.\n\n    Args:\n        config_json: Either a JSON string or a path to a JSON file specifying\n            transformations with keys:\n            - \"REMOVE\": list[str] of column prefixes to remove\n            - \"RENAME\": dict[str, str] mapping old-&gt;new column names\n            If None, a sensible default is used.\n    \"\"\"\n\n    def __init__(self, config_json=None):\n        self._df = None\n\n        # Codes from the FHIR datatype \"coding\"\n        # (f.e. element resource.code.coding or element resource.clinicalStatus.coding)\n        # are extracted to a col \"codingcodes\"\n        # (f.e. col resource.code.codingcodes or col resource.clinicalStatus.codingcodes)\n        # without other for analysis often not needed metadata like f.e. codesystem URI\n        # or FHIR extensions for coding entries.\n        # The full / raw object in col \"coding\" is deleted after this extraction.\n        # If you want to analyze more than the content of code and display from codings\n        # (like f.e. different codesystem URIs or further codes in extensions\n        # in the raw data/object), you can disable deletion of the raw source object \"coding\"\n        # (f.e. col \"resource.code.coding\") by setting property delete_col_raw_coding to False\n        self._delete_col_raw_coding = True\n        if config_json is not None:\n            try:\n                with open(config_json, \"r\") as f:  # config_json is a file path\n                    self.config = json.load(f)\n            except:\n                self.config = json.loads(config_json)  # config_json is a json string\n        else:\n            self.config = json.loads(\n                '{ \"REMOVE\": [\"resource.text.div\"], \"RENAME\": { \"resource.id\": \"id\" } }'\n            )\n\n    @property\n    def df(self):\n        \"\"\"pd.DataFrame | None: The current working dataframe, if any.\"\"\"\n        return self._df\n\n    @property\n    def delete_col_raw_coding(self):\n        \"\"\"bool: Whether to drop raw coding/display columns after extraction.\"\"\"\n        return self._delete_col_raw_coding\n\n    @delete_col_raw_coding.setter\n    def delete_col_raw_coding(self, delete_col_raw_coding):\n        \"\"\"Set whether to drop raw coding/display columns after extraction.\n\n        Args:\n            delete_col_raw_coding (bool): True to delete raw columns after creating\n                derived columns, False to keep them.\n        \"\"\"\n        self._delete_col_raw_coding = delete_col_raw_coding\n\n    def read_bundle_from_bundle_dict(self, bundle_dict):\n        \"\"\"Normalize a FHIR Bundle dict to a dataframe of entries.\n\n        Args:\n            bundle_dict (dict): A FHIR Bundle object with an \"entry\" list.\n\n        Returns:\n            pd.DataFrame: Dataframe where each row corresponds to a Bundle entry.\n        \"\"\"\n        return pd.json_normalize(bundle_dict[\"entry\"])\n\n    def delete_unwanted_cols(self):\n        \"\"\"Delete unwanted columns from the dataframe.\n\n        Uses the \"REMOVE\" list from the configuration. Any column that equals a\n        listed value or starts with that value followed by a dot will be removed.\n        Safely no-ops if the dataframe or configuration is missing.\n        \"\"\"\n        if self._df is None:\n            logger.warning(\"Dataframe is empty, nothing to delete\")\n            return\n        if \"REMOVE\" not in self.config:\n            logger.warning(\"No columns to remove defined in config\")\n            return\n        if not isinstance(self.config[\"REMOVE\"], list):\n            logger.warning(\n                \"REMOVE in config is not a list, expected a list of column names to remove\"\n            )\n            return\n        if len(self.config[\"REMOVE\"]) == 0:\n            logger.warning(\"No columns to remove defined in config\")\n            return\n\n        # Collect all columns to remove first, then drop once for better performance\n        cols_to_remove = []\n        for col in self.config[\"REMOVE\"]:\n            cols_to_remove.extend([\n                c for c in self._df.columns if c == col or c.startswith(f\"{col}.\")\n            ])\n\n        # Single drop operation for better performance\n        if cols_to_remove:\n            self._df.drop(columns=cols_to_remove, inplace=True)\n\n    def rename_cols(self):\n        \"\"\"Rename dataframe columns according to the configuration.\n\n        Uses the \"RENAME\" mapping from the configuration. Safely no-ops if the\n        dataframe is empty.\n        \"\"\"\n        if self._df is not None:\n            self._df.rename(columns=self.config[\"RENAME\"], inplace=True)\n        else:\n            logger.warning(\"Dataframe is empty, nothing to rename\")\n\n    def remove_string_from_columns(self, string_to_remove=\"resource.\"):\n        \"\"\"Remove a literal substring from all column names.\n\n        Args:\n            string_to_remove: Substring to remove from column names.\n\n        Returns:\n            pd.DataFrame | None: The updated dataframe or None if unset.\n        \"\"\"\n        if self._df is not None:\n            self._df.columns = self._df.columns.str.replace(\n                string_to_remove, \"\", regex=False\n            )\n        else:\n            logger.warning(\"Dataframe is empty, cannot remove string from columns\")\n        return self._df\n\n    def process_df(self):\n        \"\"\"Run the standard transformation pipeline on the dataframe.\n\n        Steps include:\n        - Extracting codes from coding/display objects to flat columns\n        - Adding a patientId column\n        - Removing common prefix from column names\n        - Converting empty lists to NaN\n        - Dropping empty columns\n        - Deleting unwanted columns\n        - Renaming columns per config\n\n        Returns:\n            pd.DataFrame | None: The processed dataframe, or None if unset.\n        \"\"\"\n        self.convert_object_to_list()\n        self.add_patient_id()\n        self.remove_string_from_columns(string_to_remove=\"resource.\")\n        self.empty_list_to_nan()\n        self.drop_empty_cols()\n        self.delete_unwanted_cols()\n        self.rename_cols()\n        return self._df\n\n    def empty_list_to_nan(self):\n        \"\"\"Convert empty list values in object columns to NaN.\"\"\"\n        if self._df is None:\n            logger.warning(\"Dataframe is empty, nothing to convert\")\n            return\n        for col in self._df.columns:\n            if self._df[col].dtype == \"object\":\n                # Use vectorized operation with where() for better performance\n                is_empty_list = self._df[col].apply(lambda x: isinstance(x, list) and len(x) == 0)\n                self._df[col] = self._df[col].where(~is_empty_list, float(\"nan\"))\n\n    def drop_empty_cols(self):\n        \"\"\"Drop columns that are completely empty (all NaN values).\"\"\"\n        if self._df is None:\n            logger.warning(\"Dataframe is empty, nothing to drop\")\n            return\n        self._df.dropna(axis=1, how=\"all\", inplace=True)\n        if self._df is not None and self._df.empty:\n            logger.warning(\"Dataframe is empty after dropping empty columns\")\n        return self._df\n\n    def process_bundle_dict(self, bundle_dict):\n        \"\"\"Load and process a FHIR Bundle dictionary.\n\n        Args:\n            bundle_dict (dict): A FHIR Bundle object.\n\n        Returns:\n            pd.DataFrame | None: The processed dataframe, or None if empty.\n        \"\"\"\n        self._df = self.read_bundle_from_bundle_dict(bundle_dict)\n        if self._df is None or self._df.empty:\n            logger.warning(\"Dataframe is empty, nothing to process\")\n            return None\n        self._df = self.process_df()\n        return self._df\n\n    def convert_object_to_list(self):\n        \"\"\"Extract codes/display from nested objects into flat list columns.\n\n        For columns containing \"coding\" or \"display\" in their names, extract a\n        list of codes or display texts into new columns with \".codes\" or\n        \".display\" suffixes. Optionally drops raw source columns.\n        \"\"\"\n        if self._df is None:\n            logger.warning(\"Dataframe is empty, nothing to convert\")\n            return\n\n        def _codes_comma_series(src_col: str) -&gt; pd.Series:\n            \"\"\"Return a Series with comma-separated strings from list-like values.\n\n            Args:\n                src_col: Column name to extract and stringify.\n\n            Returns:\n                pd.Series: Comma-separated strings (or empty string when None).\n            \"\"\"\n            codes = self._df[src_col].apply(self.process_list)\n            return codes.apply(\n                lambda x: (\n                    \", \".join(x)\n                    if isinstance(x, list) and x is not None\n                    else (x if x is not None else \"\")\n                )\n            )\n\n        # Collect all new columns first, then concat once for better performance\n        new_columns = {}\n        cols_to_drop = []\n\n        for col in self._df.columns:\n            if \"coding\" in col:\n                codes_as_comma_separated = _codes_comma_series(col)\n                new_columns[col + \".codes\"] = codes_as_comma_separated\n                if self._delete_col_raw_coding:\n                    cols_to_drop.append(col)\n            if \"display\" in col:\n                codes_as_comma_separated = _codes_comma_series(col)\n                new_columns[col + \".display\"] = codes_as_comma_separated\n                cols_to_drop.append(col)\n\n        # Single concat operation for all new columns\n        if new_columns:\n            self._df = pd.concat([self._df, pd.DataFrame(new_columns)], axis=1)\n\n        # Drop columns after concat\n        if cols_to_drop:\n            self._df.drop(columns=cols_to_drop, inplace=True)\n\n    def add_patient_id(self):\n        \"\"\"Add a patientId column inferred from resource fields.\n\n        If the resource type is Patient, uses the resource id; otherwise attempts\n        to derive the patient identifier from known subject/patient reference fields.\n        \"\"\"\n        if self._df is None:\n            logger.warning(\"Dataframe is empty, cannot add patientId\")\n            return\n        try:\n            # Use vectorized operations for better performance\n            # Check if resource type is Patient\n            is_patient = self._df[\"resource.resourceType\"] == \"Patient\"\n\n            # For Patient resources, use resource.id\n            patient_ids = self._df[\"resource.id\"].where(is_patient, \"\")\n\n            # For non-Patient resources, check subject/patient references\n            # Try each possible reference field in order\n            ref_keys = [\n                \"resource.subject.reference\",\n                \"resource.patient.reference\",\n            ]\n\n            for key in ref_keys:\n                if key in self._df.columns:\n                    # Get reference values where not already set\n                    ref_values = self._df[key].fillna(\"\")\n                    # Clean the reference (remove Patient/ and urn:uuid: prefixes)\n                    cleaned_refs = ref_values.str.replace(\"Patient/\", \"\", regex=False).str.replace(\"urn:uuid:\", \"\", regex=False)\n                    # Update patient_ids where empty and reference exists\n                    mask = (patient_ids == \"\") &amp; (cleaned_refs != \"\")\n                    patient_ids = patient_ids.where(~mask, cleaned_refs)\n\n            self._df[\"patientId\"] = patient_ids\n\n        except:\n            try:\n                # Fallback for resources without \"resource.\" prefix\n                is_patient = self._df[\"resourceType\"] == \"Patient\"\n                patient_ids = self._df[\"id\"].where(is_patient, \"\")\n\n                ref_keys = [\n                    \"subject.reference\",\n                    \"patient.reference\",\n                ]\n\n                for key in ref_keys:\n                    if key in self._df.columns:\n                        ref_values = self._df[key].fillna(\"\")\n                        cleaned_refs = ref_values.str.replace(\"Patient/\", \"\", regex=False).str.replace(\"urn:uuid:\", \"\", regex=False)\n                        mask = (patient_ids == \"\") &amp; (cleaned_refs != \"\")\n                        patient_ids = patient_ids.where(~mask, cleaned_refs)\n\n                self._df[\"patientId\"] = patient_ids\n            except:\n                pass\n\n    def check_subject_reference(self, row):\n        \"\"\"Extract patient id from subject/patient reference fields.\n\n        Args:\n            row (Mapping[str, Any]): A dataframe row as a mapping.\n\n        Returns:\n            str: The patient id (without \"Patient/\" or \"urn:uuid:\" prefix) or\n            an empty string if not found.\n        \"\"\"\n        keys = [\n            \"resource.subject.reference\",\n            \"resource.patient.reference\",\n            \"subject.reference\",\n            \"patient.reference\",\n        ]\n\n        def _clean(ref):\n            if not isinstance(ref, str):\n                return \"\"\n            return ref.replace(\"Patient/\", \"\").replace(\"urn:uuid:\", \"\")\n\n        for key in keys:\n            ref = row.get(key, None)\n            if pd.notna(ref):\n                return _clean(ref)\n\n        return \"\"\n\n    def get_info(self):\n        \"\"\"Return a concise info string for the current dataframe.\n\n        Returns:\n            str: Dataframe info text or a message if no dataframe is set.\n        \"\"\"\n        if self._df is None:\n            return \"Dataframe is empty\"\n        return self._df.info()\n\n    def process_list(self, myList):\n        \"\"\"Extract code or display strings from a list of coding-like dicts.\n\n        Args:\n            myList (list): A list of dictionaries that may contain \"code\" or\n                \"display\" keys.\n\n        Returns:\n            list[str]: A list of extracted codes/display texts.\n        \"\"\"\n        myCodes = []\n        if isinstance(myList, list):\n            for entry in myList:\n                if \"code\" in entry:\n                    myCodes.append(entry[\"code\"])\n                elif \"display\" in entry:\n                    myCodes.append(entry[\"display\"])\n        return myCodes\n\n    def llm_query(self, query, llm, embed_model=None, verbose=True):\n        \"\"\"Execute a natural language query against the dataframe using LLM tools.\n\n        Args:\n            query (str): The natural language question.\n            llm (Any): The language model instance usable by llama_index.\n            embed_model (str | None): Optional HuggingFace embedding model name.\n            verbose (bool): Whether to enable verbose output from the query engine.\n\n        Raises:\n            Exception: If required libraries are not installed.\n            Exception: If the dataframe is empty.\n\n        Returns:\n            Any: The query result from the underlying engine.\n        \"\"\"\n        try:\n            from langchain_huggingface import HuggingFaceEmbeddings\n            from llama_index.core import Settings\n            from llama_index.experimental.query_engine import PandasQueryEngine\n        except Exception:\n            raise Exception(\"llama_index or HuggingFaceEmbeddings not installed\")\n        if self._df is None:\n            raise Exception(\"Dataframe is empty\")\n        if embed_model is None:\n            embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n        else:\n            embed_model = HuggingFaceEmbeddings(model_name=embed_model)\n        Settings.llm = llm\n        Settings.embed_model = embed_model\n        query_engine = PandasQueryEngine(\n            df=self._df,\n            verbose=verbose,\n        )\n        return query_engine.query(query)\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.delete_col_raw_coding","title":"<code>delete_col_raw_coding</code>  <code>property</code> <code>writable</code>","text":"<p>bool: Whether to drop raw coding/display columns after extraction.</p>"},{"location":"modules/#base_fhiry.BaseFhiry.df","title":"<code>df</code>  <code>property</code>","text":"<p>pd.DataFrame | None: The current working dataframe, if any.</p>"},{"location":"modules/#base_fhiry.BaseFhiry.add_patient_id","title":"<code>add_patient_id()</code>","text":"<p>Add a patientId column inferred from resource fields.</p> <p>If the resource type is Patient, uses the resource id; otherwise attempts to derive the patient identifier from known subject/patient reference fields.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def add_patient_id(self):\n    \"\"\"Add a patientId column inferred from resource fields.\n\n    If the resource type is Patient, uses the resource id; otherwise attempts\n    to derive the patient identifier from known subject/patient reference fields.\n    \"\"\"\n    if self._df is None:\n        logger.warning(\"Dataframe is empty, cannot add patientId\")\n        return\n    try:\n        # Use vectorized operations for better performance\n        # Check if resource type is Patient\n        is_patient = self._df[\"resource.resourceType\"] == \"Patient\"\n\n        # For Patient resources, use resource.id\n        patient_ids = self._df[\"resource.id\"].where(is_patient, \"\")\n\n        # For non-Patient resources, check subject/patient references\n        # Try each possible reference field in order\n        ref_keys = [\n            \"resource.subject.reference\",\n            \"resource.patient.reference\",\n        ]\n\n        for key in ref_keys:\n            if key in self._df.columns:\n                # Get reference values where not already set\n                ref_values = self._df[key].fillna(\"\")\n                # Clean the reference (remove Patient/ and urn:uuid: prefixes)\n                cleaned_refs = ref_values.str.replace(\"Patient/\", \"\", regex=False).str.replace(\"urn:uuid:\", \"\", regex=False)\n                # Update patient_ids where empty and reference exists\n                mask = (patient_ids == \"\") &amp; (cleaned_refs != \"\")\n                patient_ids = patient_ids.where(~mask, cleaned_refs)\n\n        self._df[\"patientId\"] = patient_ids\n\n    except:\n        try:\n            # Fallback for resources without \"resource.\" prefix\n            is_patient = self._df[\"resourceType\"] == \"Patient\"\n            patient_ids = self._df[\"id\"].where(is_patient, \"\")\n\n            ref_keys = [\n                \"subject.reference\",\n                \"patient.reference\",\n            ]\n\n            for key in ref_keys:\n                if key in self._df.columns:\n                    ref_values = self._df[key].fillna(\"\")\n                    cleaned_refs = ref_values.str.replace(\"Patient/\", \"\", regex=False).str.replace(\"urn:uuid:\", \"\", regex=False)\n                    mask = (patient_ids == \"\") &amp; (cleaned_refs != \"\")\n                    patient_ids = patient_ids.where(~mask, cleaned_refs)\n\n            self._df[\"patientId\"] = patient_ids\n        except:\n            pass\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.check_subject_reference","title":"<code>check_subject_reference(row)</code>","text":"<p>Extract patient id from subject/patient reference fields.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Mapping[str, Any]</code> <p>A dataframe row as a mapping.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The patient id (without \"Patient/\" or \"urn:uuid:\" prefix) or</p> <p>an empty string if not found.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def check_subject_reference(self, row):\n    \"\"\"Extract patient id from subject/patient reference fields.\n\n    Args:\n        row (Mapping[str, Any]): A dataframe row as a mapping.\n\n    Returns:\n        str: The patient id (without \"Patient/\" or \"urn:uuid:\" prefix) or\n        an empty string if not found.\n    \"\"\"\n    keys = [\n        \"resource.subject.reference\",\n        \"resource.patient.reference\",\n        \"subject.reference\",\n        \"patient.reference\",\n    ]\n\n    def _clean(ref):\n        if not isinstance(ref, str):\n            return \"\"\n        return ref.replace(\"Patient/\", \"\").replace(\"urn:uuid:\", \"\")\n\n    for key in keys:\n        ref = row.get(key, None)\n        if pd.notna(ref):\n            return _clean(ref)\n\n    return \"\"\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.convert_object_to_list","title":"<code>convert_object_to_list()</code>","text":"<p>Extract codes/display from nested objects into flat list columns.</p> <p>For columns containing \"coding\" or \"display\" in their names, extract a list of codes or display texts into new columns with \".codes\" or \".display\" suffixes. Optionally drops raw source columns.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def convert_object_to_list(self):\n    \"\"\"Extract codes/display from nested objects into flat list columns.\n\n    For columns containing \"coding\" or \"display\" in their names, extract a\n    list of codes or display texts into new columns with \".codes\" or\n    \".display\" suffixes. Optionally drops raw source columns.\n    \"\"\"\n    if self._df is None:\n        logger.warning(\"Dataframe is empty, nothing to convert\")\n        return\n\n    def _codes_comma_series(src_col: str) -&gt; pd.Series:\n        \"\"\"Return a Series with comma-separated strings from list-like values.\n\n        Args:\n            src_col: Column name to extract and stringify.\n\n        Returns:\n            pd.Series: Comma-separated strings (or empty string when None).\n        \"\"\"\n        codes = self._df[src_col].apply(self.process_list)\n        return codes.apply(\n            lambda x: (\n                \", \".join(x)\n                if isinstance(x, list) and x is not None\n                else (x if x is not None else \"\")\n            )\n        )\n\n    # Collect all new columns first, then concat once for better performance\n    new_columns = {}\n    cols_to_drop = []\n\n    for col in self._df.columns:\n        if \"coding\" in col:\n            codes_as_comma_separated = _codes_comma_series(col)\n            new_columns[col + \".codes\"] = codes_as_comma_separated\n            if self._delete_col_raw_coding:\n                cols_to_drop.append(col)\n        if \"display\" in col:\n            codes_as_comma_separated = _codes_comma_series(col)\n            new_columns[col + \".display\"] = codes_as_comma_separated\n            cols_to_drop.append(col)\n\n    # Single concat operation for all new columns\n    if new_columns:\n        self._df = pd.concat([self._df, pd.DataFrame(new_columns)], axis=1)\n\n    # Drop columns after concat\n    if cols_to_drop:\n        self._df.drop(columns=cols_to_drop, inplace=True)\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.delete_unwanted_cols","title":"<code>delete_unwanted_cols()</code>","text":"<p>Delete unwanted columns from the dataframe.</p> <p>Uses the \"REMOVE\" list from the configuration. Any column that equals a listed value or starts with that value followed by a dot will be removed. Safely no-ops if the dataframe or configuration is missing.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def delete_unwanted_cols(self):\n    \"\"\"Delete unwanted columns from the dataframe.\n\n    Uses the \"REMOVE\" list from the configuration. Any column that equals a\n    listed value or starts with that value followed by a dot will be removed.\n    Safely no-ops if the dataframe or configuration is missing.\n    \"\"\"\n    if self._df is None:\n        logger.warning(\"Dataframe is empty, nothing to delete\")\n        return\n    if \"REMOVE\" not in self.config:\n        logger.warning(\"No columns to remove defined in config\")\n        return\n    if not isinstance(self.config[\"REMOVE\"], list):\n        logger.warning(\n            \"REMOVE in config is not a list, expected a list of column names to remove\"\n        )\n        return\n    if len(self.config[\"REMOVE\"]) == 0:\n        logger.warning(\"No columns to remove defined in config\")\n        return\n\n    # Collect all columns to remove first, then drop once for better performance\n    cols_to_remove = []\n    for col in self.config[\"REMOVE\"]:\n        cols_to_remove.extend([\n            c for c in self._df.columns if c == col or c.startswith(f\"{col}.\")\n        ])\n\n    # Single drop operation for better performance\n    if cols_to_remove:\n        self._df.drop(columns=cols_to_remove, inplace=True)\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.drop_empty_cols","title":"<code>drop_empty_cols()</code>","text":"<p>Drop columns that are completely empty (all NaN values).</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def drop_empty_cols(self):\n    \"\"\"Drop columns that are completely empty (all NaN values).\"\"\"\n    if self._df is None:\n        logger.warning(\"Dataframe is empty, nothing to drop\")\n        return\n    self._df.dropna(axis=1, how=\"all\", inplace=True)\n    if self._df is not None and self._df.empty:\n        logger.warning(\"Dataframe is empty after dropping empty columns\")\n    return self._df\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.empty_list_to_nan","title":"<code>empty_list_to_nan()</code>","text":"<p>Convert empty list values in object columns to NaN.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def empty_list_to_nan(self):\n    \"\"\"Convert empty list values in object columns to NaN.\"\"\"\n    if self._df is None:\n        logger.warning(\"Dataframe is empty, nothing to convert\")\n        return\n    for col in self._df.columns:\n        if self._df[col].dtype == \"object\":\n            # Use vectorized operation with where() for better performance\n            is_empty_list = self._df[col].apply(lambda x: isinstance(x, list) and len(x) == 0)\n            self._df[col] = self._df[col].where(~is_empty_list, float(\"nan\"))\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.get_info","title":"<code>get_info()</code>","text":"<p>Return a concise info string for the current dataframe.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>Dataframe info text or a message if no dataframe is set.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def get_info(self):\n    \"\"\"Return a concise info string for the current dataframe.\n\n    Returns:\n        str: Dataframe info text or a message if no dataframe is set.\n    \"\"\"\n    if self._df is None:\n        return \"Dataframe is empty\"\n    return self._df.info()\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.llm_query","title":"<code>llm_query(query, llm, embed_model=None, verbose=True)</code>","text":"<p>Execute a natural language query against the dataframe using LLM tools.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The natural language question.</p> required <code>llm</code> <code>Any</code> <p>The language model instance usable by llama_index.</p> required <code>embed_model</code> <code>str | None</code> <p>Optional HuggingFace embedding model name.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose output from the query engine.</p> <code>True</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If required libraries are not installed.</p> <code>Exception</code> <p>If the dataframe is empty.</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>The query result from the underlying engine.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def llm_query(self, query, llm, embed_model=None, verbose=True):\n    \"\"\"Execute a natural language query against the dataframe using LLM tools.\n\n    Args:\n        query (str): The natural language question.\n        llm (Any): The language model instance usable by llama_index.\n        embed_model (str | None): Optional HuggingFace embedding model name.\n        verbose (bool): Whether to enable verbose output from the query engine.\n\n    Raises:\n        Exception: If required libraries are not installed.\n        Exception: If the dataframe is empty.\n\n    Returns:\n        Any: The query result from the underlying engine.\n    \"\"\"\n    try:\n        from langchain_huggingface import HuggingFaceEmbeddings\n        from llama_index.core import Settings\n        from llama_index.experimental.query_engine import PandasQueryEngine\n    except Exception:\n        raise Exception(\"llama_index or HuggingFaceEmbeddings not installed\")\n    if self._df is None:\n        raise Exception(\"Dataframe is empty\")\n    if embed_model is None:\n        embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n    else:\n        embed_model = HuggingFaceEmbeddings(model_name=embed_model)\n    Settings.llm = llm\n    Settings.embed_model = embed_model\n    query_engine = PandasQueryEngine(\n        df=self._df,\n        verbose=verbose,\n    )\n    return query_engine.query(query)\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.process_bundle_dict","title":"<code>process_bundle_dict(bundle_dict)</code>","text":"<p>Load and process a FHIR Bundle dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>bundle_dict</code> <code>dict</code> <p>A FHIR Bundle object.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame | None: The processed dataframe, or None if empty.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def process_bundle_dict(self, bundle_dict):\n    \"\"\"Load and process a FHIR Bundle dictionary.\n\n    Args:\n        bundle_dict (dict): A FHIR Bundle object.\n\n    Returns:\n        pd.DataFrame | None: The processed dataframe, or None if empty.\n    \"\"\"\n    self._df = self.read_bundle_from_bundle_dict(bundle_dict)\n    if self._df is None or self._df.empty:\n        logger.warning(\"Dataframe is empty, nothing to process\")\n        return None\n    self._df = self.process_df()\n    return self._df\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.process_df","title":"<code>process_df()</code>","text":"<p>Run the standard transformation pipeline on the dataframe.</p> <p>Steps include: - Extracting codes from coding/display objects to flat columns - Adding a patientId column - Removing common prefix from column names - Converting empty lists to NaN - Dropping empty columns - Deleting unwanted columns - Renaming columns per config</p> <p>Returns:</p> Type Description <p>pd.DataFrame | None: The processed dataframe, or None if unset.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def process_df(self):\n    \"\"\"Run the standard transformation pipeline on the dataframe.\n\n    Steps include:\n    - Extracting codes from coding/display objects to flat columns\n    - Adding a patientId column\n    - Removing common prefix from column names\n    - Converting empty lists to NaN\n    - Dropping empty columns\n    - Deleting unwanted columns\n    - Renaming columns per config\n\n    Returns:\n        pd.DataFrame | None: The processed dataframe, or None if unset.\n    \"\"\"\n    self.convert_object_to_list()\n    self.add_patient_id()\n    self.remove_string_from_columns(string_to_remove=\"resource.\")\n    self.empty_list_to_nan()\n    self.drop_empty_cols()\n    self.delete_unwanted_cols()\n    self.rename_cols()\n    return self._df\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.process_list","title":"<code>process_list(myList)</code>","text":"<p>Extract code or display strings from a list of coding-like dicts.</p> <p>Parameters:</p> Name Type Description Default <code>myList</code> <code>list</code> <p>A list of dictionaries that may contain \"code\" or \"display\" keys.</p> required <p>Returns:</p> Type Description <p>list[str]: A list of extracted codes/display texts.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def process_list(self, myList):\n    \"\"\"Extract code or display strings from a list of coding-like dicts.\n\n    Args:\n        myList (list): A list of dictionaries that may contain \"code\" or\n            \"display\" keys.\n\n    Returns:\n        list[str]: A list of extracted codes/display texts.\n    \"\"\"\n    myCodes = []\n    if isinstance(myList, list):\n        for entry in myList:\n            if \"code\" in entry:\n                myCodes.append(entry[\"code\"])\n            elif \"display\" in entry:\n                myCodes.append(entry[\"display\"])\n    return myCodes\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.read_bundle_from_bundle_dict","title":"<code>read_bundle_from_bundle_dict(bundle_dict)</code>","text":"<p>Normalize a FHIR Bundle dict to a dataframe of entries.</p> <p>Parameters:</p> Name Type Description Default <code>bundle_dict</code> <code>dict</code> <p>A FHIR Bundle object with an \"entry\" list.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: Dataframe where each row corresponds to a Bundle entry.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def read_bundle_from_bundle_dict(self, bundle_dict):\n    \"\"\"Normalize a FHIR Bundle dict to a dataframe of entries.\n\n    Args:\n        bundle_dict (dict): A FHIR Bundle object with an \"entry\" list.\n\n    Returns:\n        pd.DataFrame: Dataframe where each row corresponds to a Bundle entry.\n    \"\"\"\n    return pd.json_normalize(bundle_dict[\"entry\"])\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.remove_string_from_columns","title":"<code>remove_string_from_columns(string_to_remove='resource.')</code>","text":"<p>Remove a literal substring from all column names.</p> <p>Parameters:</p> Name Type Description Default <code>string_to_remove</code> <p>Substring to remove from column names.</p> <code>'resource.'</code> <p>Returns:</p> Type Description <p>pd.DataFrame | None: The updated dataframe or None if unset.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def remove_string_from_columns(self, string_to_remove=\"resource.\"):\n    \"\"\"Remove a literal substring from all column names.\n\n    Args:\n        string_to_remove: Substring to remove from column names.\n\n    Returns:\n        pd.DataFrame | None: The updated dataframe or None if unset.\n    \"\"\"\n    if self._df is not None:\n        self._df.columns = self._df.columns.str.replace(\n            string_to_remove, \"\", regex=False\n        )\n    else:\n        logger.warning(\"Dataframe is empty, cannot remove string from columns\")\n    return self._df\n</code></pre>"},{"location":"modules/#base_fhiry.BaseFhiry.rename_cols","title":"<code>rename_cols()</code>","text":"<p>Rename dataframe columns according to the configuration.</p> <p>Uses the \"RENAME\" mapping from the configuration. Safely no-ops if the dataframe is empty.</p> Source code in <code>src/fhiry/base_fhiry.py</code> <pre><code>def rename_cols(self):\n    \"\"\"Rename dataframe columns according to the configuration.\n\n    Uses the \"RENAME\" mapping from the configuration. Safely no-ops if the\n    dataframe is empty.\n    \"\"\"\n    if self._df is not None:\n        self._df.rename(columns=self.config[\"RENAME\"], inplace=True)\n    else:\n        logger.warning(\"Dataframe is empty, nothing to rename\")\n</code></pre>"},{"location":"modules/#bqsearch.BQsearch","title":"<code>BQsearch</code>","text":"<p>               Bases: <code>BaseFhiry</code></p> <p>Query FHIR datasets in Google BigQuery and process results.</p> Source code in <code>src/fhiry/bqsearch.py</code> <pre><code>class BQsearch(BaseFhiry):\n    \"\"\"Query FHIR datasets in Google BigQuery and process results.\"\"\"\n\n    def __init__(self, config_json=None):\n        # Construct a BigQuery client object.\n        self._client = bigquery.Client()\n        super().__init__(config_json=config_json)\n\n    def search(self, query=None):\n        \"\"\"Run a BigQuery SQL query and return a processed dataframe.\n\n        Args:\n            query (str | None): Either a SQL string, a path to a .sql file, or\n                None to run a default sample query.\n\n        Returns:\n            pd.DataFrame: The query results after standard processing.\n        \"\"\"\n        if query is None:\n            _query = \"\"\"\n                SELECT *\n                FROM `bigquery-public-data.fhir_synthea.patient`\n                LIMIT 20\n            \"\"\"\n        else:\n            try:\n                with open(query, \"r\") as f:\n                    _query = f.read()\n            except:\n                _query = query\n\n        self._df = self._client.query(_query).to_dataframe()\n        super().process_df()\n        return self._df\n</code></pre>"},{"location":"modules/#bqsearch.BQsearch.search","title":"<code>search(query=None)</code>","text":"<p>Run a BigQuery SQL query and return a processed dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str | None</code> <p>Either a SQL string, a path to a .sql file, or None to run a default sample query.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: The query results after standard processing.</p> Source code in <code>src/fhiry/bqsearch.py</code> <pre><code>def search(self, query=None):\n    \"\"\"Run a BigQuery SQL query and return a processed dataframe.\n\n    Args:\n        query (str | None): Either a SQL string, a path to a .sql file, or\n            None to run a default sample query.\n\n    Returns:\n        pd.DataFrame: The query results after standard processing.\n    \"\"\"\n    if query is None:\n        _query = \"\"\"\n            SELECT *\n            FROM `bigquery-public-data.fhir_synthea.patient`\n            LIMIT 20\n        \"\"\"\n    else:\n        try:\n            with open(query, \"r\") as f:\n                _query = f.read()\n        except:\n            _query = query\n\n    self._df = self._client.query(_query).to_dataframe()\n    super().process_df()\n    return self._df\n</code></pre>"},{"location":"modules/#fhirsearch.Fhirsearch","title":"<code>Fhirsearch</code>","text":"<p>               Bases: <code>BaseFhiry</code></p> <p>Search FHIR servers and aggregate results into a dataframe.</p> <p>This client pages through FHIR search results and builds a unified pandas DataFrame using the BaseFhiry processing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>fhir_base_url</code> <code>str</code> <p>Base URL of the FHIR server (e.g., \"https://.../fhir\").</p> required <code>config_json</code> <p>Optional JSON string or file path with column transforms.</p> <code>None</code> Source code in <code>src/fhiry/fhirsearch.py</code> <pre><code>class Fhirsearch(BaseFhiry):\n    \"\"\"Search FHIR servers and aggregate results into a dataframe.\n\n    This client pages through FHIR search results and builds a unified\n    pandas DataFrame using the BaseFhiry processing pipeline.\n\n    Args:\n        fhir_base_url (str): Base URL of the FHIR server (e.g., \"https://.../fhir\").\n        config_json: Optional JSON string or file path with column transforms.\n    \"\"\"\n\n    def __init__(self, fhir_base_url, config_json=None):\n        self.fhir_base_url = fhir_base_url\n\n        # Batch size (entries per page)\n        self.page_size = 500\n\n        # Keyword arguments for HTTP(s) requests (f.e. for auth)\n        # Example parameters:\n        # Authentication: https://requests.readthedocs.io/en/latest/user/authentication/#basic-authentication\n        # Proxies: https://requests.readthedocs.io/en/latest/user/advanced/#proxies\n        # SSL Certificates: https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification\n        self.requests_kwargs = {}\n        super().__init__(config_json=config_json)\n\n    def search(self, resource_type=\"Patient\", search_parameters={}):\n        \"\"\"Search the FHIR server and return the combined results.\n\n        Args:\n            resource_type (str): FHIR resource type to search (e.g., \"Patient\").\n            search_parameters (dict): Query parameters per FHIR spec; _count is\n                auto-set to the configured page size if absent.\n\n        Returns:\n            pd.DataFrame: Combined search results across all pages.\n        \"\"\"\n\n        headers = {\"Content-Type\": \"application/fhir+json\"}\n\n        if \"_count\" not in search_parameters:\n            search_parameters[\"_count\"] = self.page_size\n\n        search_url = f\"{self.fhir_base_url}/{resource_type}\"\n        r = requests.get(\n            search_url,\n            params=search_parameters,\n            headers=headers,\n            **self.requests_kwargs,\n        )\n        r.raise_for_status()\n        bundle_dict = r.json()\n\n        if \"entry\" in bundle_dict:\n            # Collect all dataframes first, then concat once for better performance\n            dataframes = []\n            df = super().process_bundle_dict(bundle_dict)\n            if df is not None and not df.empty:\n                dataframes.append(df)\n\n            next_page_url = get_next_page_url(bundle_dict)\n\n            while next_page_url:\n                r = requests.get(next_page_url, headers=headers, **self.requests_kwargs)\n                r.raise_for_status()\n                bundle_dict = r.json()\n                df_page = super().process_bundle_dict(bundle_dict)\n                if df_page is not None and not df_page.empty:\n                    dataframes.append(df_page)\n\n                next_page_url = get_next_page_url(bundle_dict)\n\n            # Single concat operation with ignore_index for better performance\n            if dataframes:\n                df = pd.concat(dataframes, ignore_index=True)\n            else:\n                df = pd.DataFrame()\n        else:\n            df = pd.DataFrame()\n\n        self._df = df\n        return self._df\n</code></pre>"},{"location":"modules/#fhirsearch.Fhirsearch.search","title":"<code>search(resource_type='Patient', search_parameters={})</code>","text":"<p>Search the FHIR server and return the combined results.</p> <p>Parameters:</p> Name Type Description Default <code>resource_type</code> <code>str</code> <p>FHIR resource type to search (e.g., \"Patient\").</p> <code>'Patient'</code> <code>search_parameters</code> <code>dict</code> <p>Query parameters per FHIR spec; _count is auto-set to the configured page size if absent.</p> <code>{}</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Combined search results across all pages.</p> Source code in <code>src/fhiry/fhirsearch.py</code> <pre><code>def search(self, resource_type=\"Patient\", search_parameters={}):\n    \"\"\"Search the FHIR server and return the combined results.\n\n    Args:\n        resource_type (str): FHIR resource type to search (e.g., \"Patient\").\n        search_parameters (dict): Query parameters per FHIR spec; _count is\n            auto-set to the configured page size if absent.\n\n    Returns:\n        pd.DataFrame: Combined search results across all pages.\n    \"\"\"\n\n    headers = {\"Content-Type\": \"application/fhir+json\"}\n\n    if \"_count\" not in search_parameters:\n        search_parameters[\"_count\"] = self.page_size\n\n    search_url = f\"{self.fhir_base_url}/{resource_type}\"\n    r = requests.get(\n        search_url,\n        params=search_parameters,\n        headers=headers,\n        **self.requests_kwargs,\n    )\n    r.raise_for_status()\n    bundle_dict = r.json()\n\n    if \"entry\" in bundle_dict:\n        # Collect all dataframes first, then concat once for better performance\n        dataframes = []\n        df = super().process_bundle_dict(bundle_dict)\n        if df is not None and not df.empty:\n            dataframes.append(df)\n\n        next_page_url = get_next_page_url(bundle_dict)\n\n        while next_page_url:\n            r = requests.get(next_page_url, headers=headers, **self.requests_kwargs)\n            r.raise_for_status()\n            bundle_dict = r.json()\n            df_page = super().process_bundle_dict(bundle_dict)\n            if df_page is not None and not df_page.empty:\n                dataframes.append(df_page)\n\n            next_page_url = get_next_page_url(bundle_dict)\n\n        # Single concat operation with ignore_index for better performance\n        if dataframes:\n            df = pd.concat(dataframes, ignore_index=True)\n        else:\n            df = pd.DataFrame()\n    else:\n        df = pd.DataFrame()\n\n    self._df = df\n    return self._df\n</code></pre>"},{"location":"modules/#fhirsearch.get_next_page_url","title":"<code>get_next_page_url(bundle_dict)</code>","text":"<p>Return the URL of the next page from a FHIR Bundle, if present.</p> <p>Parameters:</p> Name Type Description Default <code>bundle_dict</code> <code>dict</code> <p>The FHIR Bundle JSON object.</p> required <p>Returns:</p> Type Description <p>str | None: The 'next' page URL, or None if no more pages.</p> Source code in <code>src/fhiry/fhirsearch.py</code> <pre><code>def get_next_page_url(bundle_dict):\n    \"\"\"Return the URL of the next page from a FHIR Bundle, if present.\n\n    Args:\n        bundle_dict (dict): The FHIR Bundle JSON object.\n\n    Returns:\n        str | None: The 'next' page URL, or None if no more pages.\n    \"\"\"\n    links = bundle_dict.get(\"link\")\n    if links:\n        for link in links:\n            relation = link.get(\"relation\")\n            if relation == \"next\":\n                return link.get(\"url\")\n\n    return None\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir","title":"<code>FlattenFhir</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Flatten FHIR resources to concise human-readable text.</p> <p>Parameters:</p> Name Type Description Default <code>fhirobject</code> <code>dict</code> <p>A FHIR resource or Bundle to flatten.</p> <code>{}</code> <code>config_json</code> <p>Currently unused placeholder for future options.</p> <code>None</code> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>class FlattenFhir(ABC):\n    \"\"\"Flatten FHIR resources to concise human-readable text.\n\n    Args:\n        fhirobject (dict): A FHIR resource or Bundle to flatten.\n        config_json: Currently unused placeholder for future options.\n    \"\"\"\n\n    def __init__(self, fhirobject={}, config_json=None):\n        self._flattened = \"\"\n        self._fhirobject = Prodict.from_dict(fhirobject)\n        if fhirobject:\n            self.flatten()\n\n    @property\n    def flattened(self):\n        \"\"\"str: The last flattened output string.\"\"\"\n        return self._flattened\n\n    @property\n    def fhirobject(self):\n        \"\"\"Prodict: The current FHIR object as Prodict.\"\"\"\n        return self._fhirobject\n\n    @fhirobject.setter\n    def fhirobject(self, fhirobject):\n        \"\"\"Set a FHIR object and immediately refresh the flattened output.\n\n        Args:\n            fhirobject (dict): A FHIR resource or Bundle.\n        \"\"\"\n        self._fhirobject = Prodict.from_dict(fhirobject)\n        self.flatten()\n\n    def flatten(self):\n        \"\"\"Compute the flattened text for the current FHIR object.\n\n        Returns:\n            str: The flattened string.\n        \"\"\"\n        if not self._fhirobject:\n            _logger.info(\"FHIR object is not set.\")\n            raise ValueError(\"FHIR object is not set.\")\n        self._flattened = \"\"\n        if self._fhirobject.resourceType == \"Bundle\":\n            for entry in self._fhirobject.entry:\n                _entry = Prodict.from_dict(entry)\n                self.get_flattened_text(_entry.resource)\n        else:\n            self.get_flattened_text(self._fhirobject)\n        return self._flattened\n\n    def get_flattened_text(self, entry):\n        \"\"\"Append flattened text for a single FHIR entry to the buffer.\n\n        Args:\n            entry (Prodict): A FHIR resource object.\n\n        Returns:\n            str: The updated flattened string.\n        \"\"\"\n        if entry.resourceType == \"Patient\":\n            self._flattened += self.flatten_patient(entry)\n        elif entry.resourceType == \"Observation\":\n            self._flattened += self.flatten_observation(entry)\n        elif entry.resourceType == \"Medication\":\n            self._flattened += self.flatten_medication(entry)\n        elif entry.resourceType == \"Procedure\":\n            self._flattened += self.flatten_procedure(entry)\n        elif entry.resourceType == \"Condition\":\n            self._flattened += self.flatten_condition(entry)\n        elif entry.resourceType == \"AllergyIntolerance\":\n            self._flattened += self.flatten_allergyintolerance(entry)\n        elif entry.resourceType == \"DocumentReference\":\n            self._flattened += self.flatten_documentreference(entry)\n        else:\n            _logger.info(f\"Resource type not supported: {entry.resourceType}\")\n        return self._flattened\n\n    def get_timeago(self, datestring) -&gt; str:\n        \"\"\"Return a human-friendly time-ago string for the given date.\n\n        Args:\n            datestring (str): ISO-like date string (YYYY-MM-DD...).\n\n        Returns:\n            str: Human-friendly relative time.\n        \"\"\"\n        datestring = datestring[0:10]\n        return timeago.format(datestring, datetime.datetime.now())\n\n    def flatten_patient(self, patient) -&gt; str:\n        \"\"\"Flatten a Patient into a short sentence.\n\n        Args:\n            patient: Patient resource object.\n\n        Returns:\n            str: Flattened snippet.\n        \"\"\"\n        flat_patient = \"\"\n        if \"gender\" in patient:\n            flat_patient += f\"Medical record of a {patient.gender} patient \"\n        else:\n            _logger.info(f\"Gender not found for patient {patient.id}\")\n            flat_patient += \"Medical record of a patient \"\n        if \"birthDate\" in patient:\n            flat_patient += f\"born {self.get_timeago(patient.birthDate)}. \"\n        else:\n            _logger.info(f\"Birthdate not found for patient {patient.id}\")\n            flat_patient += \"of unknown age. \"\n        return flat_patient\n\n    def flatten_observation(self, observation) -&gt; str:\n        \"\"\"Flatten an Observation into a short sentence.\"\"\"\n        flat_observation = \"\"\n        if \"code\" in observation:\n            _display = observation.code.coding[0]\n            flat_observation += f\"{_display['display']} \"\n        else:\n            _logger.info(f\"Code not found for observation {observation.id}\")\n            flat_observation += \"Observation \"\n        if \"effectiveDateTime\" in observation:\n            flat_observation += (\n                f\"recorded {self.get_timeago(observation.effectiveDateTime)} was \"\n            )\n        else:\n            _logger.info(f\"Effective date not found for observation {observation.id}\")\n            flat_observation += \"of unknown date was \"\n        if \"valueQuantity\" in observation and \"value\" in observation.valueQuantity:\n            flat_observation += f\"Value: {observation.valueQuantity.value} \"\n            if \"unit\" in observation.valueQuantity:\n                flat_observation += f\"{observation.valueQuantity.unit}. \"\n        elif \"valueString\" in observation:\n            flat_observation += f\"Value: {observation.valueString}. \"\n        elif \"valueBoolean\" in observation:\n            flat_observation += f\"Value: {observation.valueBoolean}. \"\n        elif (\n            \"valueRange\" in observation\n            and \"low\" in observation.valueRange\n            and \"high\" in observation.valueRange\n        ):\n            flat_observation += f\"Value: {observation.valueRange.low.value} - {observation.valueRange.high.value} {observation.valueRange.low.unit}. \"\n        elif (\n            \"valueRatio\" in observation\n            and \"numerator\" in observation.valueRatio\n            and \"denominator\" in observation.valueRatio\n        ):\n            flat_observation += f\"Value: {observation.valueRatio.numerator.value} {observation.valueRatio.numerator.unit} / {observation.valueRatio.denominator.value} {observation.valueRatio.denominator.unit}. \"\n        elif (\n            \"valuePeriod\" in observation\n            and \"start\" in observation.valuePeriod\n            and \"end\" in observation.valuePeriod\n        ):\n            flat_observation += f\"Value: {observation.valuePeriod.start} - {observation.valuePeriod.end}. \"\n        elif \"valueDateTime\" in observation and observation.valueDateTime != \"\":\n            flat_observation += f\"Value: {observation.valueDateTime}. \"\n        elif \"valueTime\" in observation and observation.valueTime != \"\":\n            flat_observation += f\"Value: {observation.valueTime}. \"\n        elif (\n            \"valueSampledData\" in observation and \"data\" in observation.valueSampledData\n        ):\n            flat_observation += f\"Value: {observation.valueSampledData.data}. \"\n        else:\n            _logger.info(f\"Value not found for observation {observation.id}\")\n            flat_observation += \"Value: unknown. \"\n        try:\n            if (\n                \"interpretation\" in observation\n                and \"coding\" in observation.interpretation[0]\n            ):\n                if \"coding\" in observation.interpretation[0]:\n                    _text = observation.interpretation[0][\"coding\"][0]\n                    flat_observation += f\"Interpretation: {_text['display']}. \"\n        except:\n            _logger.info(f\"Interpretation not found for observation {observation.id}\")\n            flat_observation += \"Interpretation: unknown. \"\n        return flat_observation\n\n    def flatten_medication(self, medication) -&gt; str:\n        \"\"\"Flatten a Medication into a short sentence.\"\"\"\n        flat_medication = \"\"\n        if \"code\" in medication:\n            flat_medication += f\"{medication.code.coding[0]['display']} \"\n        else:\n            _logger.info(f\"Code not found for medication {medication.id}\")\n            flat_medication += \"Medication \"\n        if \"status\" in medication:\n            flat_medication += f\"Status: {medication.status}. \"\n        else:\n            _logger.info(f\"Status not found for medication {medication.id}\")\n            flat_medication += \"Status: unknown. \"\n        return flat_medication\n\n    def flatten_procedure(self, procedure) -&gt; str:\n        \"\"\"Flatten a Procedure into a short sentence.\"\"\"\n        flat_procedure = \"\"\n        if (\n            \"code\" in procedure\n            and \"coding\" in procedure.code\n            and \"display\" in procedure.code.coding[0]\n        ):\n            flat_procedure += f\"{procedure.code.coding[0]['display']} was \"\n        else:\n            _logger.info(f\"Code not found for procedure {procedure.id}\")\n            flat_procedure += \"Procedure was\"\n        if \"occurrenceDateTime\" in procedure:\n            flat_procedure += (\n                f\"{procedure.status} {self.get_timeago(procedure.occurrenceDateTime)}. \"\n            )\n        elif \"occurrencePeriod\" in procedure:\n            flat_procedure += f\"{procedure.status} {self.get_timeago(procedure.occurrencePeriod.start)}. \"\n        else:\n            _logger.info(f\"Performed date not found for procedure {procedure.id}\")\n            flat_procedure += \"on unknown date. \"\n        return flat_procedure\n\n    def flatten_condition(self, condition) -&gt; str:\n        \"\"\"Flatten a Condition into a short sentence.\"\"\"\n        flat_condition = \"\"\n        if \"code\" in condition:\n            flat_condition += f\"{condition.code.coding[0]['display']} \"\n        else:\n            _logger.info(f\"Code not found for condition {condition.id}\")\n            flat_condition += \"Condition \"\n        if condition.onsetDateTime:\n            flat_condition += (\n                f\"was diagnosed {self.get_timeago(condition.onsetDateTime)}. \"\n            )\n        else:\n            _logger.info(f\"Onset date not found for condition {condition.id}\")\n            flat_condition += \"was diagnosed. \"\n        return flat_condition\n\n    def flatten_allergyintolerance(self, allergyintolerance) -&gt; str:\n        \"\"\"Flatten an AllergyIntolerance into a short sentence.\"\"\"\n        flat_allergyintolerance = \"\"\n        _display = allergyintolerance.code.coding[0]\n        if \"code\" in allergyintolerance and \"display\" in _display:\n            flat_allergyintolerance += f\"{_display['display']} \"\n        else:\n            _logger.info(\n                f\"Code not found for allergyintolerance {allergyintolerance.id}\"\n            )\n            flat_allergyintolerance += \"AllergyIntolerance \"\n        if \"onsetDateTime\" in allergyintolerance:\n            flat_allergyintolerance += f\" allergy was reported on {self.get_timeago(allergyintolerance.onsetDateTime)}. \"\n        else:\n            _logger.info(\n                f\"Onset date not found for allergyintolerance {allergyintolerance.id}\"\n            )\n            flat_allergyintolerance += \"allergy reported. \"\n        return flat_allergyintolerance\n\n    def flatten_documentreference(self, documentreference) -&gt; str:\n        \"\"\"Flatten a DocumentReference into a short sentence.\"\"\"\n        flat_documentreference = \"\"\n        for content in documentreference.content:\n            content = Prodict.from_dict(content)\n            if content.attachment.contentType == \"text/plain\":\n                flat_documentreference += (\n                    f\"{content.attachment.title}: {content.attachment.data}\"\n                )\n            else:\n                _logger.info(\n                    f\"Attachment for documentreference {documentreference.id} is not text/plain.\"\n                )\n        if \"date\" in documentreference:\n            flat_documentreference += (\n                f\" was created {self.get_timeago(documentreference.date)}. \"\n            )\n        else:\n            _logger.info(f\"Date not found for documentreference {documentreference.id}\")\n            flat_documentreference += \" was created. \"\n        return flat_documentreference\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.fhirobject","title":"<code>fhirobject</code>  <code>property</code> <code>writable</code>","text":"<p>Prodict: The current FHIR object as Prodict.</p>"},{"location":"modules/#flattenfhir.FlattenFhir.flattened","title":"<code>flattened</code>  <code>property</code>","text":"<p>str: The last flattened output string.</p>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten","title":"<code>flatten()</code>","text":"<p>Compute the flattened text for the current FHIR object.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The flattened string.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten(self):\n    \"\"\"Compute the flattened text for the current FHIR object.\n\n    Returns:\n        str: The flattened string.\n    \"\"\"\n    if not self._fhirobject:\n        _logger.info(\"FHIR object is not set.\")\n        raise ValueError(\"FHIR object is not set.\")\n    self._flattened = \"\"\n    if self._fhirobject.resourceType == \"Bundle\":\n        for entry in self._fhirobject.entry:\n            _entry = Prodict.from_dict(entry)\n            self.get_flattened_text(_entry.resource)\n    else:\n        self.get_flattened_text(self._fhirobject)\n    return self._flattened\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten_allergyintolerance","title":"<code>flatten_allergyintolerance(allergyintolerance)</code>","text":"<p>Flatten an AllergyIntolerance into a short sentence.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten_allergyintolerance(self, allergyintolerance) -&gt; str:\n    \"\"\"Flatten an AllergyIntolerance into a short sentence.\"\"\"\n    flat_allergyintolerance = \"\"\n    _display = allergyintolerance.code.coding[0]\n    if \"code\" in allergyintolerance and \"display\" in _display:\n        flat_allergyintolerance += f\"{_display['display']} \"\n    else:\n        _logger.info(\n            f\"Code not found for allergyintolerance {allergyintolerance.id}\"\n        )\n        flat_allergyintolerance += \"AllergyIntolerance \"\n    if \"onsetDateTime\" in allergyintolerance:\n        flat_allergyintolerance += f\" allergy was reported on {self.get_timeago(allergyintolerance.onsetDateTime)}. \"\n    else:\n        _logger.info(\n            f\"Onset date not found for allergyintolerance {allergyintolerance.id}\"\n        )\n        flat_allergyintolerance += \"allergy reported. \"\n    return flat_allergyintolerance\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten_condition","title":"<code>flatten_condition(condition)</code>","text":"<p>Flatten a Condition into a short sentence.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten_condition(self, condition) -&gt; str:\n    \"\"\"Flatten a Condition into a short sentence.\"\"\"\n    flat_condition = \"\"\n    if \"code\" in condition:\n        flat_condition += f\"{condition.code.coding[0]['display']} \"\n    else:\n        _logger.info(f\"Code not found for condition {condition.id}\")\n        flat_condition += \"Condition \"\n    if condition.onsetDateTime:\n        flat_condition += (\n            f\"was diagnosed {self.get_timeago(condition.onsetDateTime)}. \"\n        )\n    else:\n        _logger.info(f\"Onset date not found for condition {condition.id}\")\n        flat_condition += \"was diagnosed. \"\n    return flat_condition\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten_documentreference","title":"<code>flatten_documentreference(documentreference)</code>","text":"<p>Flatten a DocumentReference into a short sentence.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten_documentreference(self, documentreference) -&gt; str:\n    \"\"\"Flatten a DocumentReference into a short sentence.\"\"\"\n    flat_documentreference = \"\"\n    for content in documentreference.content:\n        content = Prodict.from_dict(content)\n        if content.attachment.contentType == \"text/plain\":\n            flat_documentreference += (\n                f\"{content.attachment.title}: {content.attachment.data}\"\n            )\n        else:\n            _logger.info(\n                f\"Attachment for documentreference {documentreference.id} is not text/plain.\"\n            )\n    if \"date\" in documentreference:\n        flat_documentreference += (\n            f\" was created {self.get_timeago(documentreference.date)}. \"\n        )\n    else:\n        _logger.info(f\"Date not found for documentreference {documentreference.id}\")\n        flat_documentreference += \" was created. \"\n    return flat_documentreference\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten_medication","title":"<code>flatten_medication(medication)</code>","text":"<p>Flatten a Medication into a short sentence.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten_medication(self, medication) -&gt; str:\n    \"\"\"Flatten a Medication into a short sentence.\"\"\"\n    flat_medication = \"\"\n    if \"code\" in medication:\n        flat_medication += f\"{medication.code.coding[0]['display']} \"\n    else:\n        _logger.info(f\"Code not found for medication {medication.id}\")\n        flat_medication += \"Medication \"\n    if \"status\" in medication:\n        flat_medication += f\"Status: {medication.status}. \"\n    else:\n        _logger.info(f\"Status not found for medication {medication.id}\")\n        flat_medication += \"Status: unknown. \"\n    return flat_medication\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten_observation","title":"<code>flatten_observation(observation)</code>","text":"<p>Flatten an Observation into a short sentence.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten_observation(self, observation) -&gt; str:\n    \"\"\"Flatten an Observation into a short sentence.\"\"\"\n    flat_observation = \"\"\n    if \"code\" in observation:\n        _display = observation.code.coding[0]\n        flat_observation += f\"{_display['display']} \"\n    else:\n        _logger.info(f\"Code not found for observation {observation.id}\")\n        flat_observation += \"Observation \"\n    if \"effectiveDateTime\" in observation:\n        flat_observation += (\n            f\"recorded {self.get_timeago(observation.effectiveDateTime)} was \"\n        )\n    else:\n        _logger.info(f\"Effective date not found for observation {observation.id}\")\n        flat_observation += \"of unknown date was \"\n    if \"valueQuantity\" in observation and \"value\" in observation.valueQuantity:\n        flat_observation += f\"Value: {observation.valueQuantity.value} \"\n        if \"unit\" in observation.valueQuantity:\n            flat_observation += f\"{observation.valueQuantity.unit}. \"\n    elif \"valueString\" in observation:\n        flat_observation += f\"Value: {observation.valueString}. \"\n    elif \"valueBoolean\" in observation:\n        flat_observation += f\"Value: {observation.valueBoolean}. \"\n    elif (\n        \"valueRange\" in observation\n        and \"low\" in observation.valueRange\n        and \"high\" in observation.valueRange\n    ):\n        flat_observation += f\"Value: {observation.valueRange.low.value} - {observation.valueRange.high.value} {observation.valueRange.low.unit}. \"\n    elif (\n        \"valueRatio\" in observation\n        and \"numerator\" in observation.valueRatio\n        and \"denominator\" in observation.valueRatio\n    ):\n        flat_observation += f\"Value: {observation.valueRatio.numerator.value} {observation.valueRatio.numerator.unit} / {observation.valueRatio.denominator.value} {observation.valueRatio.denominator.unit}. \"\n    elif (\n        \"valuePeriod\" in observation\n        and \"start\" in observation.valuePeriod\n        and \"end\" in observation.valuePeriod\n    ):\n        flat_observation += f\"Value: {observation.valuePeriod.start} - {observation.valuePeriod.end}. \"\n    elif \"valueDateTime\" in observation and observation.valueDateTime != \"\":\n        flat_observation += f\"Value: {observation.valueDateTime}. \"\n    elif \"valueTime\" in observation and observation.valueTime != \"\":\n        flat_observation += f\"Value: {observation.valueTime}. \"\n    elif (\n        \"valueSampledData\" in observation and \"data\" in observation.valueSampledData\n    ):\n        flat_observation += f\"Value: {observation.valueSampledData.data}. \"\n    else:\n        _logger.info(f\"Value not found for observation {observation.id}\")\n        flat_observation += \"Value: unknown. \"\n    try:\n        if (\n            \"interpretation\" in observation\n            and \"coding\" in observation.interpretation[0]\n        ):\n            if \"coding\" in observation.interpretation[0]:\n                _text = observation.interpretation[0][\"coding\"][0]\n                flat_observation += f\"Interpretation: {_text['display']}. \"\n    except:\n        _logger.info(f\"Interpretation not found for observation {observation.id}\")\n        flat_observation += \"Interpretation: unknown. \"\n    return flat_observation\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten_patient","title":"<code>flatten_patient(patient)</code>","text":"<p>Flatten a Patient into a short sentence.</p> <p>Parameters:</p> Name Type Description Default <code>patient</code> <p>Patient resource object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Flattened snippet.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten_patient(self, patient) -&gt; str:\n    \"\"\"Flatten a Patient into a short sentence.\n\n    Args:\n        patient: Patient resource object.\n\n    Returns:\n        str: Flattened snippet.\n    \"\"\"\n    flat_patient = \"\"\n    if \"gender\" in patient:\n        flat_patient += f\"Medical record of a {patient.gender} patient \"\n    else:\n        _logger.info(f\"Gender not found for patient {patient.id}\")\n        flat_patient += \"Medical record of a patient \"\n    if \"birthDate\" in patient:\n        flat_patient += f\"born {self.get_timeago(patient.birthDate)}. \"\n    else:\n        _logger.info(f\"Birthdate not found for patient {patient.id}\")\n        flat_patient += \"of unknown age. \"\n    return flat_patient\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.flatten_procedure","title":"<code>flatten_procedure(procedure)</code>","text":"<p>Flatten a Procedure into a short sentence.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def flatten_procedure(self, procedure) -&gt; str:\n    \"\"\"Flatten a Procedure into a short sentence.\"\"\"\n    flat_procedure = \"\"\n    if (\n        \"code\" in procedure\n        and \"coding\" in procedure.code\n        and \"display\" in procedure.code.coding[0]\n    ):\n        flat_procedure += f\"{procedure.code.coding[0]['display']} was \"\n    else:\n        _logger.info(f\"Code not found for procedure {procedure.id}\")\n        flat_procedure += \"Procedure was\"\n    if \"occurrenceDateTime\" in procedure:\n        flat_procedure += (\n            f\"{procedure.status} {self.get_timeago(procedure.occurrenceDateTime)}. \"\n        )\n    elif \"occurrencePeriod\" in procedure:\n        flat_procedure += f\"{procedure.status} {self.get_timeago(procedure.occurrencePeriod.start)}. \"\n    else:\n        _logger.info(f\"Performed date not found for procedure {procedure.id}\")\n        flat_procedure += \"on unknown date. \"\n    return flat_procedure\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.get_flattened_text","title":"<code>get_flattened_text(entry)</code>","text":"<p>Append flattened text for a single FHIR entry to the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>Prodict</code> <p>A FHIR resource object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The updated flattened string.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def get_flattened_text(self, entry):\n    \"\"\"Append flattened text for a single FHIR entry to the buffer.\n\n    Args:\n        entry (Prodict): A FHIR resource object.\n\n    Returns:\n        str: The updated flattened string.\n    \"\"\"\n    if entry.resourceType == \"Patient\":\n        self._flattened += self.flatten_patient(entry)\n    elif entry.resourceType == \"Observation\":\n        self._flattened += self.flatten_observation(entry)\n    elif entry.resourceType == \"Medication\":\n        self._flattened += self.flatten_medication(entry)\n    elif entry.resourceType == \"Procedure\":\n        self._flattened += self.flatten_procedure(entry)\n    elif entry.resourceType == \"Condition\":\n        self._flattened += self.flatten_condition(entry)\n    elif entry.resourceType == \"AllergyIntolerance\":\n        self._flattened += self.flatten_allergyintolerance(entry)\n    elif entry.resourceType == \"DocumentReference\":\n        self._flattened += self.flatten_documentreference(entry)\n    else:\n        _logger.info(f\"Resource type not supported: {entry.resourceType}\")\n    return self._flattened\n</code></pre>"},{"location":"modules/#flattenfhir.FlattenFhir.get_timeago","title":"<code>get_timeago(datestring)</code>","text":"<p>Return a human-friendly time-ago string for the given date.</p> <p>Parameters:</p> Name Type Description Default <code>datestring</code> <code>str</code> <p>ISO-like date string (YYYY-MM-DD...).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Human-friendly relative time.</p> Source code in <code>src/fhiry/flattenfhir.py</code> <pre><code>def get_timeago(self, datestring) -&gt; str:\n    \"\"\"Return a human-friendly time-ago string for the given date.\n\n    Args:\n        datestring (str): ISO-like date string (YYYY-MM-DD...).\n\n    Returns:\n        str: Human-friendly relative time.\n    \"\"\"\n    datestring = datestring[0:10]\n    return timeago.format(datestring, datetime.datetime.now())\n</code></pre>"},{"location":"modules/#fhirndjson.Fhirndjson","title":"<code>Fhirndjson</code>","text":"<p>               Bases: <code>BaseFhiry</code></p> <p>Read and process NDJSON FHIR resources from a folder.</p> <p>Parameters:</p> Name Type Description Default <code>config_json</code> <p>Optional JSON string or file path with column transforms.</p> <code>None</code> Source code in <code>src/fhiry/fhirndjson.py</code> <pre><code>class Fhirndjson(BaseFhiry):\n    \"\"\"Read and process NDJSON FHIR resources from a folder.\n\n    Args:\n        config_json: Optional JSON string or file path with column transforms.\n    \"\"\"\n\n    def __init__(self, config_json=None):\n        self._folder = \"\"\n        super().__init__(config_json=config_json)\n\n    @property\n    def df(self):\n        \"\"\"pd.DataFrame | None: The current working dataframe, if any.\"\"\"\n        return self._df\n\n    @property\n    def folder(self):\n        \"\"\"str: The folder containing NDJSON files to process.\"\"\"\n        return self._folder\n\n    @folder.setter\n    def folder(self, folder):\n        \"\"\"Set the NDJSON input folder.\n\n        Args:\n            folder (str): Path to a directory with .ndjson files.\n        \"\"\"\n        self._folder = folder\n\n    def read_resource_from_line(self, line):\n        \"\"\"Normalize a single NDJSON line (JSON object) to a dataframe row.\"\"\"\n        return pd.json_normalize(json.loads(line))\n\n    def process_source(self):\n        \"\"\"Process all NDJSON files in the folder into a single dataframe.\n\n        Only columns common across resources will be mapped.\n        \"\"\"\n        if self._folder:\n            for file in tqdm(os.listdir(self._folder)):\n                self.process_file(file)\n\n    def process_file(self, file):\n        \"\"\"Process a single NDJSON file and append its rows to the dataframe.\n\n        Args:\n            file (str): Filename within the configured folder to process.\n\n        Returns:\n            pd.DataFrame | None: The updated dataframe.\n        \"\"\"\n        if file.endswith(\".ndjson\"):\n            dataframes = []\n            if self._df is not None and not self._df.empty:\n                dataframes.append(self._df)\n\n            with open(os.path.join(self._folder, file)) as fp:\n                Lines = fp.readlines()\n                # Collect all dataframes first, then concat once for better performance\n                for line in tqdm(Lines):\n                    self._df = self.read_resource_from_line(line)\n                    self.process_df()\n                    if not self._df.empty:\n                        dataframes.append(self._df)\n\n            # Single concat operation with ignore_index for better performance\n            if dataframes:\n                self._df = pd.concat(dataframes, ignore_index=True)\n            else:\n                self._df = pd.DataFrame()\n        return self._df\n</code></pre>"},{"location":"modules/#fhirndjson.Fhirndjson.df","title":"<code>df</code>  <code>property</code>","text":"<p>pd.DataFrame | None: The current working dataframe, if any.</p>"},{"location":"modules/#fhirndjson.Fhirndjson.folder","title":"<code>folder</code>  <code>property</code> <code>writable</code>","text":"<p>str: The folder containing NDJSON files to process.</p>"},{"location":"modules/#fhirndjson.Fhirndjson.process_file","title":"<code>process_file(file)</code>","text":"<p>Process a single NDJSON file and append its rows to the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Filename within the configured folder to process.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame | None: The updated dataframe.</p> Source code in <code>src/fhiry/fhirndjson.py</code> <pre><code>def process_file(self, file):\n    \"\"\"Process a single NDJSON file and append its rows to the dataframe.\n\n    Args:\n        file (str): Filename within the configured folder to process.\n\n    Returns:\n        pd.DataFrame | None: The updated dataframe.\n    \"\"\"\n    if file.endswith(\".ndjson\"):\n        dataframes = []\n        if self._df is not None and not self._df.empty:\n            dataframes.append(self._df)\n\n        with open(os.path.join(self._folder, file)) as fp:\n            Lines = fp.readlines()\n            # Collect all dataframes first, then concat once for better performance\n            for line in tqdm(Lines):\n                self._df = self.read_resource_from_line(line)\n                self.process_df()\n                if not self._df.empty:\n                    dataframes.append(self._df)\n\n        # Single concat operation with ignore_index for better performance\n        if dataframes:\n            self._df = pd.concat(dataframes, ignore_index=True)\n        else:\n            self._df = pd.DataFrame()\n    return self._df\n</code></pre>"},{"location":"modules/#fhirndjson.Fhirndjson.process_source","title":"<code>process_source()</code>","text":"<p>Process all NDJSON files in the folder into a single dataframe.</p> <p>Only columns common across resources will be mapped.</p> Source code in <code>src/fhiry/fhirndjson.py</code> <pre><code>def process_source(self):\n    \"\"\"Process all NDJSON files in the folder into a single dataframe.\n\n    Only columns common across resources will be mapped.\n    \"\"\"\n    if self._folder:\n        for file in tqdm(os.listdir(self._folder)):\n            self.process_file(file)\n</code></pre>"},{"location":"modules/#fhirndjson.Fhirndjson.read_resource_from_line","title":"<code>read_resource_from_line(line)</code>","text":"<p>Normalize a single NDJSON line (JSON object) to a dataframe row.</p> Source code in <code>src/fhiry/fhirndjson.py</code> <pre><code>def read_resource_from_line(self, line):\n    \"\"\"Normalize a single NDJSON line (JSON object) to a dataframe row.\"\"\"\n    return pd.json_normalize(json.loads(line))\n</code></pre>"},{"location":"modules/#parallel.ndjson","title":"<code>ndjson(folder, config_json=None)</code>","text":"<p>Process many NDJSON files in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Directory path or a single file path.</p> required <code>config_json</code> <p>Optional JSON string or file path with column transforms.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Concatenated dataframe across all processed files.</p> Source code in <code>src/fhiry/parallel.py</code> <pre><code>def ndjson(folder, config_json=None):\n    \"\"\"Process many NDJSON files in parallel.\n\n    Args:\n        folder (str): Directory path or a single file path.\n        config_json: Optional JSON string or file path with column transforms.\n\n    Returns:\n        pd.DataFrame: Concatenated dataframe across all processed files.\n    \"\"\"\n    logger.info(\"CPU count: {}\".format(mp.cpu_count()))\n    f = Fhirndjson(config_json=config_json)\n    filenames = []\n\n    if os.path.isdir(folder):\n        for filename in os.listdir(folder):\n            if filename.endswith(\".ndjson\"):\n                filenames.append(folder + \"/\" + filename)\n    else:\n        filenames.append(folder)\n\n    with mp.Pool(mp.cpu_count()) as pool:\n        list_of_dataframes = list(\n            tqdm(\n                pool.imap(f.process_file, filenames),\n                total=len(filenames),\n                desc=\"Processing NDJSON files\",\n            )\n        )\n    # Filter out empty dataframes and use ignore_index for better performance\n    list_of_dataframes = [df for df in list_of_dataframes if not df.empty]\n    if not list_of_dataframes:\n        return pd.DataFrame()\n    return pd.concat(list_of_dataframes, ignore_index=True)\n</code></pre>"},{"location":"modules/#parallel.process","title":"<code>process(folder, config_json=None)</code>","text":"<p>Process many Bundle JSON files in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Directory path or a single file path.</p> required <code>config_json</code> <p>Optional JSON string or file path with column transforms.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Concatenated dataframe across all processed files.</p> Source code in <code>src/fhiry/parallel.py</code> <pre><code>def process(folder, config_json=None):\n    \"\"\"Process many Bundle JSON files in parallel.\n\n    Args:\n        folder (str): Directory path or a single file path.\n        config_json: Optional JSON string or file path with column transforms.\n\n    Returns:\n        pd.DataFrame: Concatenated dataframe across all processed files.\n    \"\"\"\n    logger.info(\"CPU count: {}\".format(mp.cpu_count()))\n    f = Fhiry(config_json=config_json)\n    filenames = []\n    if os.path.isdir(folder):\n        for filename in os.listdir(folder):\n            if filename.endswith(\".json\"):\n                filenames.append(folder + \"/\" + filename)\n    else:\n        filenames.append(folder)\n\n    with mp.Pool(mp.cpu_count()) as pool:\n        list_of_dataframes = list(\n            tqdm(\n                pool.imap(f.process_file, filenames),\n                total=len(filenames),\n                desc=\"Processing JSON files\",\n            )\n        )\n    # Filter out empty dataframes and use ignore_index for better performance\n    list_of_dataframes = [df for df in list_of_dataframes if not df.empty]\n    if not list_of_dataframes:\n        return pd.DataFrame()\n    return pd.concat(list_of_dataframes, ignore_index=True)\n</code></pre>"}]}